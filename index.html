<!DOCTYPE HTML>
<!--
	Stellar by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
		<!-- Global Site Tag (gtag.js) - Google Analytics -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=UA-79453179-2"></script>
		<script>
		  window.dataLayer = window.dataLayer || [];
		  function gtag(){dataLayer.push(arguments)};
		  gtag('js', new Date());

		  gtag('config', 'UA-79453179-2');
		</script>
		<script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" id="MathJax-script"></script>

		<title>Workshop on Functional Inference and Machine Intelligence 2023</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="assets/css/main.css" />
		<!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
	</head>
	<body>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header" class="alt">
						<!-- <span class="logo"><img src="images/logo.svg" alt="" /></span> -->
						<h1 style="color:white">Workshop on Functional Inference and Machine Intelligence</h1>
						<h3 style="color:white">Tokyo/online (Hybrid), March 14-16, 2023.</h3>

					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul>
							<li><a href="#home" class="active">Home</a></li>
							<li><a href="#program">Program</a></li>
<!--							<li><a href="#poster">Poster Session</a></li>-->
<!--							<li><a href="#schedule">Schedule</a></li>-->
							<li><a href="#organizers">Organizers</a></li>
							<li><a href="#location">Access</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Introduction -->
							<section id="home" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Home</h2>
										</header>
										<p>The <strong>Workshop on Functional Inference and Machine Intelligence (FIMI)</strong> is an international workshop on machine learning and statistics, with a particular focus on theory, methods, and practice. It consists of invited talks, and poster sessions are also planned.  The topics include (but not limited to):</p>
										<ul>
											<li>Machine Learning Methods</li>
											<li>Deep Learning</li>
											<li>Kernel Methods</li>
											<li>Probabilistic Methods</li>
										</ul>
										<p>The workshop will be hybrid. All schedules are in Japan Standard Time (GMT+9).</p>
										<!--Access information to the online platform will be available a week before the workshop date.-->
										
										<div Align="center">								
										<p><b>
										A registration form will be opened soon.</br>
										</b></p>
									<!--  href="https://fimi2022.gakkai.online/pre_register/new" 
										<a target="_blank" class="square_btn">
											<span>Registration (closed)</span>
										</a>-->
										</div>
										
										<div Align="center">
											
										<p>Previous Workshop: 
											<a href="https://sites.google.com/site/2016pgm/" target="_blank">2016</a>, 
											<a href="https://sites.google.com/site/2017pgm/home" target="_blank">2017</a>, 
											<a href="https://ismseminar.github.io/fimi2018/" target="_blank">2018</a>, 
											<a href="https://ismseminar.github.io/fimi2019/" target="_blank">2019</a>,
											<a href="https://ismseminar.github.io/fimi2020/" target="_blank">2020</a>,
											<a href="https://ismseminar.github.io/fimi2021/" target="_blank">2021</a></br>
											<a href="https://ismseminar.github.io/fimi2022/" target="_blank">2022</a></br>
											<img src='images/IMG_1213.JPG' alt='' width="40%" align="middle" hspace="5%"></p>
										</div>
									</div>
								</div>

							<div class="content">
								<header class="major">
									<h2>Invited Speakers</h2>
								</header>
							<ul class="alt2">
							<li>
							<a href="https://www.gatsby.ucl.ac.uk/~gretton/" target="_blank"><b>Arthur Gretton</b></a> (University College London)</br>
							</li>
							<li>
							<a href="https://www.mathematik.tu-darmstadt.de/fb/personal/details/sophie_langer.de.jsp" target="_blank"><b>Sophie Langer</b></a> (Technical University of Darmstadt)</br>
							</li>
							<li>
							<a href="https://sites.google.com/view/shosonoda/home" target="_blank"><b>Sho Sonoda</b></a> (RIKEN Advanced Intelligence Project)</br>
							</li>
							<li>
							<a href="https://yutomiyatake.github.io" target="_blank"><b>Yuto Miyatake</b></a> (Osaka University)</br>
							</li>
							<li>
							<a href="http://ibis.t.u-tokyo.ac.jp/suzuki/" target="_blank"><b>Taiji Suzuki</b></a> (The University of Tokyo)</br>
							</li>
							<li>
							<a href="https://www.ism.ac.jp/~fukumizu/" target="_blank"><b>Kenji Fukumizu</b></a> (The Institute of Statistical Mathematics)</br>
							</li>
							<li>
							<a href="https://sites.google.com/view/mimaizumi/home" target="_blank"><b>Masaaki Imaizumi</b></a> (The University of Tokyo)</br>
							</li>
							<li>
							<a href="https://www.microsoft.com/en-us/research/people/gregyang/" target="_blank"><b>Greg Yang</b></a> (Microsoft Research)</br>
							</li>
							<li>
							<a href="https://allmodelsarewrong.net" target="_blank"><b>Song Liu</b></a> (University of Bristol)</br>
							</li>
							<li>
							<a href="https://hermite.jp" target="_blank"><b>Han Bao</b></a> (Kyoto University)</br>
							</li>
							<li>
							<a href="https://ai.stanford.edu/~tengyuma/" target="_blank"><b>Tengyu Ma</b></a> (Stanford University)</br>
							</li>
							<li>
							<a href="https://sejdino.github.io" target="_blank"><b>Dino Sejdinovic</b></a> (University of Adelaide)</br>
							</li>
							<li>
							<a href="https://kaba-lab.org/en/" target="_blank"><b>Yoshiyuki Kabashima</b></a> (The University of Tokyo)</br>
							</li>
							<li>
							<a href="https://takaosa.github.io/index.html" target="_blank"><b>Takayuki Osa</b></a> (The University of Tokyo)</br>
							</li>
							</ul>
							We are inviting more speakers.
							</div>
							</section>
							<section id="program" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Program</h2>
										</header>
									</div>
								</div>
                            				TBW
<!--						

								<div class="table-wrapper">
									<table>
										<tbody>
											<tr>
												<td colspan="2"><h2><b>Tuesday 29th March.</b></h2></td>
											</tr>
											<tr>
												<td>09:55-10:00</td>
												<td>Opening</td>
											</tr>
											<tr>
												<td>10:00-11:00</td>
												<td>
												<div class="hidden_box">
												<p>
												<a href="https://tyliang.github.io/Tengyuan.Liang/" target="_blank"><b>Tengyuan Liang</b></a>  (The University of Chicago)
                                                </br>
												Title: Universal Prediction Band, Semi-Definite Programming and Variance Interpolation</br>
												<label for="label_1_1" style="display: inline-block; _display: inline;">Abstract</label>
												</p>
												<input type="checkbox" id="label_1_1"/>
												<div class="hidden_show">
													<h5>We propose a computationally efficient method to construct nonparametric, heteroscedastic prediction bands for uncertainty quantification, with or without any user-specified predictive model. Our approach provides an alternative to the now-standard conformal prediction for uncertainty quantification, with novel theoretical insights and computational advantages. The data-adaptive prediction band is universally applicable with minimal distributional assumptions, has strong non-asymptotic coverage properties, and is easy to implement using standard convex programs. Our approach can be viewed as a novel variance interpolation with confidence and further leverages techniques from semi-definite programming and sum-of-squares optimization. Theoretical and numerical performances for the proposed approach for uncertainty quantification are analyzed.</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>11:15-12:15</td>
												<td>
											<div class="hidden_box">
												<p>
												<a href="https://sites.google.com/view/mimaizumi/" target="_blank"><b>Masaaki Imaizumi</b></a>  (The University of Tokyo)
                                                </br>
												Title: Stability of Deep Network Estimator for Nonparametric Regression with Adversarial Training</br>
												<label for="label_1_2" style="display: inline-block; _display: inline;">Abstract</label>
												</p>
												<input type="checkbox" id="label_1_2"/>
												<div class="hidden_show">
													<h5>We study the stability of an estimator for the nonparametric regression problem by deep neural networks and adversarial training. Several studies show that deep neural networks give estimators for the nonparametric problem which theoretically outperforms conventional estimators in a specific setting. A limitation of the estimator by deep networks is stability: its convergence is measured by a restricted class of norms. In this study, we consider the adversarial training for deep networks and develop an estimator for the nonparametric regression problem. We investigate its efficiency by the minimax optimization scheme and derive several convergence rates with different norms. We also discuss an application based on the result.</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>12:15-13:45</td>
												<td>Lunch break</td>
											</tr>
											<tr>
												<td>13:45-14:45</td>
												<td>
											<div class="hidden_box">
												<p>
												<a href="http://ibis.t.u-tokyo.ac.jp/suzuki/" target="_blank"><b>Taiji Suzuki</b></a>  (University of Tokyo)
                                                </br>
												Title: Effect of feature learning ability of neural networks for high dimensional input</br>
												<label for="label_1_3" style="display: inline-block; _display: inline;">Abstract</label></p>
												<input type="checkbox" id="label_1_3"/>
												<div class="hidden_show">
													<h5>
														In this talk, we discuss the benefit of adaptivity or the feature learning ability of deep learning especially for high dimensional (or even infinite dimensional) input. 
														This talk consists of two parts:</br> 
														&nbsp;(1) learning ability of neural network for infinite dimensional input with anisotropic smoothness, and </br>
														&nbsp;(2) feature learning by one step gradient descent in high dimensions.</br> 
														In the first half, I will discuss the adaptivity of deep learning to the smoothness of the true function. Although the standard nonparametric convergence is exponentially affected by the input dimensionality, 
														we prove that deep learning can avoid the curse of dimensionality when the true function has anisotropic smoothness, that is, 
														it has different smoothness towards different directions. 
														This yields superior performance of DNN compared to linear estimators including kernel methods. 
														Interestingly, this argument can be extended to infinite dimensional input. In the second half, 
														we analyze how gradient descent captures informative features and improves the generalization performance in a two-layer neural network. 
														We show that the internal layer's feature "aligns" to the true function in the first few gradient steps, 
														and precisely characterize the benefit of this alignment in the high-dimensional regime. 
														We show that the ridge estimator on trained features has improved performance, and under large step size, the learned kernel may outperform many random features and rotationally invariant kernel models. 
														This demonstrates that even one gradient step can lead to considerable advantage over the initial features.</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>15:00-16:00</td>
												<td>
											<div class="hidden_box">
												<p>
												<a href="https://riken-yamada.github.io/" target=""_blank><b>Makoto Yamada</b></a> (Kyoto University)
												</br>
												Title: Selective inference with Kernels</br>
												<label for="label_1_4" style="display: inline-block; _display: inline;">Abstract</label></p>
												<input type="checkbox" id="label_1_4"/>
												<div class="hidden_show">
													<h5>Finding a set of statistically significant features from complex data (e.g., nonlinear and/or multi-dimensional output data) is important for scientific discovery and has many practical applications, including biomarker discovery. In this talk, I introduce kernel-based selective inference frameworks that can be used to find a set of statistically significant features from non-linearly related data without splitting the data for selection and inference. Specifically, I introduce a selective variant of hypothesis testing framework based on post selection inference: two sample test with Maximum Mean Discrepancy (MMD), an independence test with Hilbert-Schmidt Independence Criterion (HSIC), a goodness of fit with Kernel Stein Discrepancy (KSD). For example, in the selective independence test, we propose the hsicInf algorithm, which can handle non-linearity and/or multi-variate/multi-class outputs through kernels. Then, I show applications of kernel-based selective inference algorithms and discuss potential future work. The talk will be an overview of our recent ICML, NeurIPS, and AISTATS publications.</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>16:15-17:15</td>
												<td>
											<div class="hidden_box">
												<p>
												<a href="https://www.beam2d.net/" target="_blank"><b>Seiya Tokui</b></a> (Preferred Networks / The University of Tokyo)
												</br>
												Title: Disentanglement Analysis with Partial Information Decomposition</br>
												<label for="label_1_5" style="display: inline-block; _display: inline;">Abstract</label>
                                                </p>
												<input type="checkbox" id="label_1_5"/>
												<div class="hidden_show">
													<h5>Disentangled representation learning is an approach to recovering the underlying factors of variation in data. While the concept is intuitive, it is far from obvious to measure how a learned representation disentangles the factors. Recently, several metrics have been proposed, which compare how each variable explains a generative factor. These metrics, however, may fail to detect entanglement that involves more than two variables, e.g., representations that duplicate and rotate generative factors in high dimensional spaces. In this talk, we introduce a framework and a metric to analyze information sharing in a multivariate representation with Partial Information Decomposition. The framework enables us to understand disentanglement in terms of uniqueness, redundancy, and synergy. We design entanglement attacks to inject multi-variable entanglement to representations and show that our framework correctly responds to entanglement. We show, through experiments on variational autoencoders, that models with similar disentanglement scores have a variety of characteristics in entanglement, for each of which a distinct strategy may be required to obtain a disentangled representation.</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>17:30-18:30</td>
												<td>
											<div class="hidden_box">
												<p>
												<a href="http://www.gatsby.ucl.ac.uk/~gretton/" target="_blank"><b>Arthur Gretton</b></a>  (University College London)
												</br>
												Title: Causal modelling with distribution embeddings: treatment effects, counterfactuals, mediation, and proxies</br>
												<label for="label_1_6" style="display: inline-block; _display: inline;">Abstract</label>
                                                </p>
												<input type="checkbox" id="label_1_6"/>
												<div class="hidden_show">
													<h5>
													A fundamental causal modelling task is to predict the effect of an intervention (or treatment) \(D=d\) on outcome \(Y\) in the presence of observed covariates \(X\). 
													We can obtain an average treatment effect by marginalising our estimate \(\gamma(X,D)\) of the conditional mean \(E(Y|X,D)\) over \(P(X)\). 
													More complex causal questions require taking conditional expectations. 
													For instance, the average treatment on the treated (ATT) addresses a counterfactual: 
													what is the outcome of an intervention \(d'\) on a subpopulation that received treatment \(d\)? 
													In this case, we must marginalise \(\gamma\) over the conditional distribution \(P(X\backslash d)\), 
													which becomes challenging for continuous multivariate \(d\). 
													Many additional causal questions require us to marginalise over conditional distributions, 
													including Conditional ATE, mediation analysis, dynamic treatment effects, and correction for unobserved confounders using proxy variables.
													We address these questions in the nonparametric setting using kernel methods, 
													which apply for very general treatments \(D\) and covariates \(X\) (learned NN features may also be used). 
													We perform marginalization over conditional distributions using conditional mean embeddings, 
													in a generalization of two-stage least-squares regression. 
													We provide strong statistical guarantees under general smoothness assumptions, 
													and a straightforward and robust implementation (a few lines of code). 
													The method is mostly demonstrated by addressing causal modelling questions arising from the US Job Corps program for Disadvantaged Youth.</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>20:00-21:30</td>
												<td><b>Poster Session</b></td>
											</tr>
											<tr>
												<td colspan="2"><h2><b>Wednesday 30th March.</b></h2></td>
											</tr>
											<tr>
												<td>10:00-11:00</td>
												<td>
											<div class="hidden_box">
												<p>
												<a href="http://www.cs.toronto.edu/~erdogdu/" target="_blank"><b>Murat A. Erdogdu</b></a> (University of Toronto)
												</br>
												Title: Analysis of Langevin Monte Carlo from Poincare to Log-Sobolev</br>
												<label for="label_2_1" style="display: inline-block; _display: inline;">Abstract</label></p>
												<input type="checkbox" id="label_2_1"/>
												<div class="hidden_show">
													<h5>
														We study sampling from a target distribution \(e^{-f}\) using the Langevin Monte Carlo (LMC) algorithm. 
														For any potential function \(f\) whose tails behave like \(|x|^\alpha\) for \(\alpha \in [1,2]\), 
														and has \(\beta\)-H&ouml;lder continuous gradient, 
														we derive the sufficient number of steps to reach the \(\varepsilon\)-neighborhood of a \(d\)-dimensional target distribution 
														as a function of \(\alpha\) and \(\beta\) in R&eacute;nyi divergence. 
														Our result is the first convergence guarantee for LMC under a functional inequality interpolating between the 
														Poincar&eacute; and log-Sobolev settings (also covering the edge cases).
													</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>11:15-12:15</td>
												<td>
											<div class="hidden_box">
												<p>
												<a href="https://sites.google.com/site/atsushinitanda" target="_blank"><b>Atsushi Nitanda</b></a> (Kyushu Institute of Technology)
												</br>
												Title: Convex Analysis of the Mean Field Langevin Dynamics</br>
												<label for="label_2_2" style="display: inline-block; _display: inline;">Abstract</label>
												</p>
												<input type="checkbox" id="label_2_2"/>
												<div class="hidden_show">
													<h5>
														As an example of the nonlinear Fokker-Planck equation, 
														the <i>mean field Langevin dynamics</i> recently attracts attention due to its connection to (noisy) gradient descent on infinitely wide neural networks in the mean field regime, 
														and hence the convergence property of the dynamics is of great theoretical interest. 
														In this talk, we give a concise convergence rate analysis of the mean field Langevin dynamics with respect to the (regularized) objective function in both continuous and discrete time settings 
														(this result in continuous time setting was also given in the concurrent and independent work [Chizat (2022)]). 
														The key ingredient of our proof is a <i>proximal Gibbs distribution</i> associated with the dynamics, 
														which, in combination with techniques in [Vempala and Wibisono (2019)], 
														allows us to develop a simple convergence theory parallel to classical results in convex optimization. 
														Furthermore, we reveal that the proximal Gibbs distribution connects to the duality gap in the empirical risk minimization setting, 
														which enables efficient empirical evaluation of the algorithm convergence.</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>12:15-15:00</td>
												<td>Lunch break</td>
											</tr>
											<tr>
												<td>15:00-16:00</td>
												<td>
											<div class="hidden_box">
												<p>
												<a href="https://sites.google.com/view/yuichi-ike" target="_blank"><b>Yuichi Ike</b></a> (University of Tokyo)
												</br>
												Title: Topological loss functions and topological representation learning</br>
												<label for="label_2_3" style="display: inline-block; _display: inline;">Abstract</label>
                                                </p>
												<input type="checkbox" id="label_2_3"/>
												<div class="hidden_show">
													<h5>
														Topological data analysis (TDA) is the branch of data science that aims to extract the topological information of given data. 
														It compactly encodes the topological features into persistence diagrams, 
														which are multisets in the two-dimensional space. In connection with machine learning, 
														many techniques have been developed to incorporate persistence diagrams into loss functions for controlling the topology of parameters. 
														In this talk, I discuss several recent developments of TDA-based loss functions and a theoretical guarantee for the convergence of such functions with respect to stochastic subgradient descent. 
														I also talk about some attempts to construct a neural network to estimate (vectorization of) persistence diagrams from data.</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>16:15-17:15</td>
												<td>
											<div class="hidden_box">
												<p>
												<a href="https://masakat0.github.io/" target="_blank"><b>Masahiro Kato</b></a> (University of Tokyo, CyberAgent, Inc.)
												</br>
												Title: Recent Findings on Density-Ratio Approaches in Machine Learning</br>
												<label for="label_2_4" style="display: inline-block; _display: inline;">Abstract</label></p>
												<input type="checkbox" id="label_2_4"/>
												<div class="hidden_show">
													<h5>
														Approaches using density ratio functions play an important role in various areas of machine learning, 
														including divergence among probability measures, anomaly detection, causal inference, and multi-armed bandit problems. 
														In this talk, I present our recent theoretical and methodological findings related to density ratios. 
														First, I report that the maximum likelihood estimation of density ratios can be interpreted as the computation of integral probability metrics (IPMs). 
														Based on this finding, we propose the density-ratio metrics (DRMs) as new divergences, which bridge the Kullback-Leibler divergence and integral probability metrics. 
														This finding also gives some insights into the estimation procedure of density ratios, such as the necessity of smoothness penalties. 
														Next, I introduce a new causal inference method by applying density ratio estimation. 
														In particular, I focus on nonparametric structural model estimation under conditional moment restrictions and find that we can solve this problem through approximation of the conditional moment restrictions using an estimated density ratio. 
														Finally, I introduce a new density-ratio-based perspective on the best-arm identification (BAI) problem. 
														In this study, we propose novel large deviation principles and develop an asymptotically optimal BAI strategy.</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>17:30-18:30</td>
												<td>
											<div class="hidden_box">
												<p>
												<a href="https://allmodelsarewrong.net" target="_blank"><b>Song Liu</b></a>  (University of Bristol)
												</br>
												Title: \(f\)-divergence and Loss Functions in ROC Curve</br>
												<label for="label_2_5" style="display: inline-block; _display: inline;">Abstract</label></p>
												<input type="checkbox" id="label_2_5"/>
												<div class="hidden_show">
													<h5>
														Given two data distributions and a test score function, 
														the Receiver Operating Characteristic (ROC) curve shows how well such a score separates two distributions. 
														However, 
														can the ROC curve be used as a measure of discrepancy between two distributions? 
														This paper shows that when the data likelihood ratio is used as the test score, 
														the arc length of the ROC curve gives rise to a novel \(f\)-divergence measuring the differences between two data distributions. 
														Approximating this arc length using a variational objective and empirical samples leads to empirical risk minimization with previously unknown loss functions. 
														We provide a Lagrangian dual objective and introduce kernel models into the estimation problem. 
														We study the non-parametric convergence rate of this estimator and show under mild smoothness conditions of the real arctangent density ratio function, 
														the rate of convergence is \(Op(n-\beta/4)\) (\(\beta\in(0,1]\) depends on the smoothness).</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>20:00-21:30</td>
												<td><b>Poster Session</b></td>
											</tr>
											<tr>
												<td colspan="2"><h2><b>Wednesday 31th March.</b></h2></td>
											</tr>
											<tr>
												<td>8:30-9:30</td>
												<td>
											<div class="hidden_box">
												<p>
												<a href="https://people.csail.mit.edu/oct/" target="_blank"><b>Octavian Ganea</b></a> (Massachusetts Institute of Technology)
												</br>
												Title: Euclidean Deep Learning Models for 3D Structures and Interactions of Molecules</br>
												<label for="label_3_1" style="display: inline-block; _display: inline;">Abstract</label></p>
												<input type="checkbox" id="label_3_1"/>
												<div class="hidden_show">
													<h5>
														Understanding 3D structures and interactions of biological nano-machines, such as proteins or drug-like molecules, 
														is crucial for assisting drug and therapeutics discovery. A core problem is molecular docking, 
														i.e., determining how two proteins or a protein and a drug-molecule attach and create a molecular complex. 
														Having access to very fast computational docking tools would enable applications such as fast virtual search for drugs inhibiting disease proteins, 
														in silico molecular design, or rapid drug side-effect prediction. However, 
														existing computer models follow a very time-consuming strategy of sampling a large number (e.g., millions) of molecular complex candidates, 
														followed by scoring, ranking, and fine-tuning steps. 
														In this talk, I will show that geometry and deep learning (DL) can significantly reduce the enormous search space associated with the docking and molecular conformation problems. 
														I will present my recent DL architectures, EquiDock and EquiBind, that perform a direct shot prediction of the molecular complex, and GeoMol, that models molecular flexibility. 
														I will argue that the governing laws of geometry, physics, or chemistry that naturally constrain these 3D structures should be incorporated in DL solutions in a mathematically meaningful way. 
														I will explain our key modeling concepts such as \(SE(3)\)-equivariant graph matching networks, attention keypoint sets, optimal transport for binding pocket prediction, 
														and torsion angle neural networks. 
														These approaches reduce the inference runtimes of open-source or commercial software from tens of minutes or hours to a few seconds, 
														while being competitive or better in terms of quality. 
														Finally, I will highlight a number of exciting on-going and future efforts in the space of artificial intelligence for structural biology and chemistry.</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>9:45-10:45</td>
												<td>
											<div class="hidden_box">
												<p>
												<b>Yang Li</b> (University of Tokyo)
												</br>
												Title: Toward robust non-rigid reconstruction with deep learning</br>
												<label for="label_3_2" style="display: inline-block; _display: inline;">Abstract</label>
                                                </p>
												<input type="checkbox" id="label_3_2"/>
												<div class="hidden_show">
													<h5>
														4D reconstruction of non-rigidly deforming scenes has numerous applications in computer vision, virtual/augmented reality, robotics, etc. 
														With the latest advancements of consumer-level depth sensors, such as Microsoft Kinect, Intel RealSense, and even smartphone-mounted cameras, non-rigid reconstruction using a single RGB-D camera has gained momentum. 
														However, due to the high complexity and non-convexity of the problem and the limitation of range sensors, 
														a robust reconstruction system for generic non-rigidly deforming scenes remains a challenge. 
														We propose learning based methods that improve the robustness of non-rigid reconstruction under unconstrained environments: </br>
														&nbsp;1) a learning-based optimization to alleviate the nonconvexity of the no-rigid tracking. </br>
														&nbsp;2) a divide and conquer strategy that can handle the non-rigid reconstruction of complex scenes with both foreground and background objects.</br> 
														&nbsp;3) a completion approach to jointly recover the occluded structure and motion from partial RGB-D sensor observation.</br> 
														&nbsp;4) a robust feature matching approach that provides reliable landmarks for global motion localization.</br> 
														Empirical experiments demonstrate the robustness and advantage of our approches over existing methods.</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>12:15-15:00</td>
												<td>Lunch break</td>
											</tr>
											<tr>
												<td>15:00-16:00</td>
												<td>
											<div class="hidden_box">
												<p>
												<a href="http://www.na.scitec.kobe-u.ac.jp/~yaguchi/" target="_blank"><b>Takaharu Yaguchi</b></a> (Kobe University)
												</br>
												Title: Geometric Deep Energy-Based Models for Physics</br>
												<label for="label_3_3" style="display: inline-block; _display: inline;">Abstract</label>
                                                </p>
												<input type="checkbox" id="label_3_3"/>
												<div class="hidden_show">
													<h5>
														Many physical phenomena can be described by energy-based models such as the Hamilton equation and phase-field models, 
														which admit the energy conservation or dissipation laws. 
														Recently, methods to construct such models from observed data using deep learning have been attracting much attention. 
														In this talk, we introduce discrete-time deep-learning models that preserve the energy conservation or dissipation laws. 
														In addition, most existing methods for the Hamilton equation assume that the data are represented in canonical coordinates. 
														However, this coordinate system is generally unknown, and this has been an obstacle for application to real problems. 
														We also introduce a geometric approach to address this problem.</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>16:15-17:15</td>
												<td>
											<div class="hidden_box">
												<p>
												<a href="https://sites.google.com/view/hinohide/" target="_blank"><b>Hideitsu Hino</b></a>  (The Institute of Statistical Mathematics)
												</br>
												Title: Symplectic integrator via contact geometry for Nesterov-type ODE</br>
												<label for="label_3_4" style="display: inline-block; _display: inline;">Abstract</label>
												</p>
												<input type="checkbox" id="label_3_4"/>
												<div class="hidden_show">
													<h5>
														We derive an explicit stable integrator based on symplectic and contact geometries for a non-autonomous ordinarily differential equation (ODE), 
														which was found in improving the convergence rate of Nesterov’s accelerated gradient method. 
														A previously investigated non-autonomous ODE is found to be able to be written as a contact Hamiltonian system. 
														Then, by developing and applying a symplectization of a non-autonomous contact Hamiltonian vector field expressing the non-autonomous ODE, 
														a symplectic integrator is derived. Because the proposed symplectic integrators preserve hidden symplectic and contact structures in the ODE, 
														they should be more stable than the standard Runge?Kutta method. Numerical experiments demonstrate that, as expected, 
														the second-order symplectic integrator is stable and high convergence rates are achieved. 
														This work is done in collaboration with Prof. Shin-itiro Goto.</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>17:30-18:30</td>
												<td>
											<div class="hidden_box">
												<p>
												<a href="http://www.ism.ac.jp/~fukumizu/" target="_blank"><b>Kenji Fukumizu</b></a>  (The Institute of Statistical Mathematics)
												</br>
												Title: A Scaling Law for Synthetic-to-real Transfer Learning</br>
												<label for="label_3_5" style="display: inline-block; _display: inline;">Abstract</label>
												</p>
												<input type="checkbox" id="label_3_5"/>
												<div class="hidden_show">
													<h5>
														Synthetic-to-real transfer learning is a framework in which a synthetically generated dataset is used to pre-train a model to improve its performance on real vision tasks. 
														The most significant advantage of using synthetic images is that the ground-truth labels are automatically available, enabling unlimited expansion of the data size without human cost. 
														However, synthetic data may have a huge domain gap, in which case increasing the data size does not improve the performance. 
														How can we know that? 
														In this study, we derive a simple scaling law that predicts the performance from the amount of pre-training data. 
														By estimating the parameters of the law, we can judge whether we should increase the data or change the setting of image synthesis. 
														Further, we analyze the theory of transfer learning by considering learning dynamics and confirm that the derived generalization bound is consistent with our empirical findings. 
														We empirically validated our scaling law on various experimental settings of benchmark tasks, model sizes, and complexities of synthetic images.</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>19:00-21:00</td>
												<td>Social Networking (@gather.town)</td>
											</tr>

										</tbody>
									</table>
								</div>
-->							
								</section>

							<section id="organizers" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Organizers</h2>
										</header>
											<ul class="alt2">
											<li>
												Masaaki Imaizumi, The University of Tokyo
											</li>
											<li>
												Taiji Suzuki, The University of Tokyo
											</li>
											<li>
												Kenji Fukumizu, The Institute of Statistical Mathematics
											</li>
											<li>
												Tatsuya Harada, The University of Tokyo
											</li>
											</ul>
									</div>
								</div>	
									
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Sponsors</h2>
										</header>
											<p>This workshop is supported by the following institution and grant:
											<li>
												<a href="http://www.ism.ac.jp/noe/sml-center/en/index.html" target="_blank">Research Center for Statistical Machine Learning, The Institute of Statistical Mathematics</a>
											</li>
											<li>
												<a href="https://www.nedo.go.jp/english/index.html" target="_blank">Japan Science and Technology Agency, CREST</a>
											</li>
												<ul style="margin: 0 0 0 0;list-style-type: none;"><li style="line-height:20px;"><span style="font-size : smaller;">
												"Innovation of Deep Structured Models with Representation of Mathematical Intelligence" 
												in 
												"Creating information utilization platform by integrating mathematical and information sciences, and development to society"</span></li></ul>
											<li>
												<a href="https://www.u-tokyo.ac.jp/en/" target="_blank">The University of Tokyo</a>
											</li>
											</ul>
											</p>
									</div>
								</div>
							</section>
					

							<section id="location" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Location</h2>
										</header>		

										<table>
										<tr>
										<td width="50%">
										
										<img src="images/build.jpg" width="100%" align="middle">
										</td>
										<td>
										<p>Address: The Institute of Statistical Mathematics</br>
										10-3 Midori-cho, Tachikawa, Tokyo 190-8562, Japan.</br>
										For detail, see <a href="http://www.ism.ac.jp/access/index_e.html" target="_blank">official access information</a>.</p>	
											
										</td>
										</tr>
										</table>
										
										<iframe src="https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d25916.50711608945!2d139.39133387198493!3d35.71236043577579!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x6018e10973c7adc1%3A0x9bad210c909fdb3e!2sThe+Institute+of+Statistical+Mathematics!5e0!3m2!1sen!2sjp!4v1506870290564" width="100%" height="300em" frameborder="0" style="border:0" allowfullscreen></iframe>
									</div>
								</div>
							</section>


					</div>

				<!-- Footer
					<footer id="footer">
						<p class="copyright">Copyright &copy; At?l?m Gune? Baydin. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
					</footer>  -->
			</div>

		<!-- Scripts  -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="assets/js/main.js"></script>
	</body>
</html>

