<!DOCTYPE HTML>
<!--
	Stellar by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
		<!-- Global Site Tag (gtag.js) - Google Analytics -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=UA-79453179-2"></script>
		<script>
		  window.dataLayer = window.dataLayer || [];
		  function gtag(){dataLayer.push(arguments)};
		  gtag('js', new Date());

		  gtag('config', 'UA-79453179-2');
		</script>
		<script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" id="MathJax-script"></script>

		<title>Workshop on Functional Inference and Machine Intelligence 2024</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="assets/css/main.css" />
		<!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
	</head>
	<body>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header" class="alt">
						<!-- <span class="logo"><img src="images/logo.svg" alt="" /></span> -->
						<h1 style="color:white">Workshop on Functional Inference and Machine Intelligence</h1>
						<h3 style="color:white">Bristol, UK, March 25-27, 2024.</h3>

					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul>
							<li><a href="#home" class="active">Home</a></li>
							<li><a href="#program">Program</a></li>
<!--							<li><a href="#poster">Poster Session</a></li>-->
<!--							<li><a href="#schedule">Schedule</a></li>-->
							<li><a href="#organizers">Organizers</a></li>
							<li><a href="#location">Access</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Introduction -->
							<section id="home" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Home</h2>
										</header>
										<div Align="center">								
										<a href="https://www.eventbrite.com/cc/functional-inference-and-machine-intelligence-3019829" 
											target="_blank" class="square_btn">
												<span>Registration (in person)</span>
											</a>
										</div>
										<p>The <strong>Workshop on Functional Inference and Machine Intelligence (FIMI)</strong> is an international workshop on machine learning and statistics, with a particular focus on theory, methods, and practice. It consists of invited talks, and poster sessions are also tentatively planned.  The topics include (but not limited to):</p>
										<ul>
											<li>Machine Learning Methods</li>
											<li>Deep Learning</li>
											<li>Kernel Methods</li>
											<li>Probabilistic Methods</li>
										</ul>
										<!--Access information to the online platform will be available a week before the workshop date.-->
																				
										
										<div Align="center">
											
										<p>Previous Workshop: 
											<a href="https://sites.google.com/site/2016pgm/" target="_blank">2016</a> 
											<a href="https://sites.google.com/site/2017pgm/home" target="_blank">2017</a> 
											<a href="https://ismseminar.github.io/fimi2018/" target="_blank">2018</a> 
											<a href="https://ismseminar.github.io/fimi2019/" target="_blank">2019</a> 
											<a href="https://ismseminar.github.io/fimi2020/" target="_blank">2020</a> 
											<a href="https://ismseminar.github.io/fimi2021/" target="_blank">2021</a> 
											<a href="https://ismseminar.github.io/fimi2022/" target="_blank">2022</a> 
											<a href="https://ismseminar.github.io/fimi2023/" target="_blank">2023</a></br>
											<img src='images/IMG_1213.JPG' alt='' width="40%" align="middle" hspace="5%"></p>
										</div>
									</div>
								</div>

<!--
							<div class="content">
								<header class="major">
									<h2>Invited Speakers</h2>
								</header>
							<ul class="alt2">
							<li>
							<a href="https://www.gatsby.ucl.ac.uk/~gretton/" target="_blank"><b>Arthur Gretton</b></a> (University College London)</br>
							</li>
							<li>
							<a href="http://ibis.t.u-tokyo.ac.jp/suzuki/" target="_blank"><b>Taiji Suzuki</b></a> (The University of Tokyo)</br>
							</li>
							<li>
							<a href="https://www.ism.ac.jp/~fukumizu/" target="_blank"><b>Kenji Fukumizu</b></a> (The Institute of Statistical Mathematics)</br>
							</li>
							<li>
							<a href="https://sites.google.com/view/mimaizumi/home" target="_blank"><b>Masaaki Imaizumi</b></a> (The University of Tokyo)</br>
							</li>
							<li>
							<a href="https://fxbriol.github.io" target="_blank"><b>François-Xavier Briol</b></a> (University College London)</br>
							</li>
							<li>
							<a href="https://www.rd.ntt/e/ns/qos/person/hashimoto/" target="_blank"><b>Yuka Hashimoto</b></a> (NTT)</br>
							</li>
							<li>
							<a href="https://www.bristol.ac.uk/people/person/Christophe-Andrieu-c8ca0164-8cd7-4fff-a864-8c6c6783a161/" target="_blank"><b>Christophe Andrieu</b></a> (University of Bristol)</br>
							</li>
							<li>
							<a href="https://mkolar.coffeejunkies.org" target="_blank"><b>Mladen Kolar</b></a> (University of Chicago)</br>
							</li>
							<li>
							<a href="http://www.gatsby.ucl.ac.uk/~laurence/" target="_blank"><b>Laurence Aitchison</b></a> (University of Bristol)</br>
							</li>
							<li>
							<a href="https://0-scholar-google-com.brum.beds.ac.uk/citations?user=juZXbFoAAAAJ&hl=no" target="_blank"><b>Jiaxin Shi</b></a> (Deepmind)</br>
							</li>
							<li>
							<a href="https://perso.telecom-paristech.fr/fdalche/" target="_blank"><b>Florence d'Alché-Buc</b></a> (Télécom Paris)</br>
							</li>
							<li>
							<a href="https://www.ceremade.dauphine.fr/~xian/" target="_blank"><b>Christian P. Robert</b></a> (Université Paris-Dauphine)</br>
							</li>
							<li>
							<a href="https://scholar.google.com/citations?user=SDhzsXUAAAAJ&hl=en" target="_blank"><b>Juliette Unwin</b></a> (University of Bristol)</br>
							</li>
							<li>
							<a href="https://sethrf.com" target="_blank"><b>Seth Flaxman</b></a> (University of Oxford)</br>
							</li>
							<li>
							<a href="https://www.imperial.ac.uk/people/s.filippi" target="_blank"><b>Sarah Filipi</b></a> (Imperial College London)</br>
							</li>
							</ul>
							</div>
							</section>
-->
							<section id="program" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Program</h2>
										</header>
									</div>
								</div>
								
								<div class="table-wrapper">
									<table>
										<tbody>
											<tr>
												<td colspan="2"><h2><b>25th March.</b></h2></td>
											</tr>
											<tr>
												<td>10:30-11:20</td>
												<td>
												<div class="hidden_box">
												<p>	
												<a href="http://www.ism.ac.jp/~fukumizu/" target="_blank"><b>Kenji Fukumizu</b></a>  (The Institute of Statistical Mathematics)
                                                </br>
												Title: Extended Flow Matching: A Method of Conditional Generation with a Matrix Field	</br>
												<label for="label_1_1" style="display: inline-block; _display: inline;">Abstract</label>
												</p>
												<input type="checkbox" id="label_1_1"/>
												<div class="hidden_show">
													<h5>The task of conditional generation is one of the most important applications of generative models, and numerous methods have been developed to date based on the celebrated diffusion models, with the guidance-based classifier-free method taking the lead.  However, the theory of the guidance-based method not only requires the user to fine-tune the ``guidance strength'', but its target vector field does not necessarily correspond to the conditional distribution used in the training. In this paper, we develop the theory of conditional generation based on Flow Matching, a current strong contender of diffusion methods.  Motivated by the interpretation of a probability path as a distribution on path space, we establish a novel theory of flow-based generation of conditional distribution by  employing the mathematical framework of generalized continuity equation instead of the continuity equation in flow matching. This theory naturally derives a method that aims to match the matrix field as opposed to the vector field.    Our framework ensures the continuity of the generated conditional distribution through the existence of flow between conditional distributions.  We will present our theory through experiments and mathematical results.  													</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>11:30-12:20</td>
												<td>
											<div class="hidden_box">
												<p>
												<a href="https://www.rd.ntt/e/ns/qos/person/hashimoto/" target="_blank"><b>Yuka Hashimoto</b></a>  (NTT)
                                                </br>
												Title: Reproducing kernel Hilbert C*-module for data analysis</br>
												<label for="label_1_2" style="display: inline-block; _display: inline;">Abstract</label>
												</p>
												<input type="checkbox" id="label_1_2"/>
												<div class="hidden_show">
													<h5>Reproducing kernel Hilbert C*-module (RKHM) is a generalization of Reproducing kernel Hilbert space (RKHS) and is characterized by a C*-algebra-valued positive definite kernel and the inner product induced by this kernel. The advantages of applying RKHMs instead of RKHSs are that we can enlarge representation spaces, construct positive definite kernels using the product structure in the C*-algebra, and use the operator norm for theoretical analyses. We show fundamental properties in RKHMs, such as representer theorems. Then, we propose a deep RKHM, which is constructed as the composition of multiple RKHMs. This framework is valid, for example, for analyzing image data.</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>12:20-14:00</td>
												<td>Lunch break</td>
											</tr>
											<tr>
												<td>14:00-14:50</td>
												<td>
											<div class="hidden_box">
												<p>
												<a href="https://perso.telecom-paristech.fr/fdalche/" target="_blank"><b>TBA</b></a>  
                                                </br>
												Title: TBA</br>
												<label for="label_1_3" style="display: inline-block; _display: inline;">Abstract</label></p>
												<input type="checkbox" id="label_1_3"/>
												<div class="hidden_show">
													<h5>TBA</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>15:00-15:50</td>
												<td>
											<div class="hidden_box">
												<p>
												<a href="https://www.ceremade.dauphine.fr/~xian/" target="_blank"><b>Christian P. Robert</b></a> (Université Paris-Dauphine)
												</br>
												Title: Sampling advances by adaptive regenerative processes and importance Monte Carlo</br>
												<label for="label_1_4" style="display: inline-block; _display: inline;">Abstract</label></p>
												<input type="checkbox" id="label_1_4"/>
												<div class="hidden_show">
													<h5>This talk will cover two recent advances in sampling, achieved in collaboration with Arthur McKimm, Murray Pollock, Gareth Roberts, Andi Wang, and with Charly Andral, Randal Douc, Hugo Marival, respectively. Enriching Brownian motion with regenerations from a fixed regeneration distribution $\mu$ at a particular regeneration rate $\kappa$ results in a Markov process that has a target distribution $\pi$ as its invariant distribution \cite{wang2021}. For the purpose of Monte Carlo inference, implementing such a scheme requires firstly selection of regeneration distribution $\mu$, and secondly computation of a specific constant $C$. Both of these tasks can be very difficult in practice for good performance. In \cite{kimm2024}, We introduce a method for adapting the regeneration distribution, by adding point masses to it. This allows the process to be simulated with as few regenerations as possible and obviates the need to find said constant $C$. Moreover, the choice of fixed $\mu$ is replaced with the choice of the initial regeneration distribution, which is considerably less difficult. We establish convergence of this resulting self-reinforcing process and explore its effectiveness at sampling from a number of target distributions. The examples show that adapting the regeneration distribution guards against poor choices of fixed regeneration distribution and can reduce the error of Monte Carlo estimates of expectations of interest, especially when $\pi$ is skewed. The Importance Markov chain is a novel algorithm proposed by \cite{andral2024} bridging the gap between rejection sampling and importance sampling, moving from one to the other through a tuning parameter. Based on a modified sample of an instrumental Markov chain targeting an instrumental distribution (typically via a MCMC kernel), the Importance Markov chain produces an extended Markov chain where the marginal distribution of the first component converges to the target distribution. For example, when targeting a multimodal distribution, the instrumental distribution can be chosen as a tempered version of the target which allows the algorithm to explore its modes more efficiently. We obtain a Law of Large Numbers and a Central Limit Theorem as well as geometric ergodicity for this extended kernel under mild assumptions on the instrumental kernel. Computationally, the algorithm is easy to implement and preexisting librairies can be used to sample from the instrumental distribution.</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>16:00-16:50</td>
												<td>
											<div class="hidden_box">
												<p>
												<a href="https://www.bristol.ac.uk/people/person/Christophe-Andrieu-c8ca0164-8cd7-4fff-a864-8c6c6783a161/" target="_blank"><b>Christophe Andrieu</b></a> (University of Bristol)
												</br>
												Title: Monte Carlo sampling with integrator snippets</br>
												<label for="label_1_5" style="display: inline-block; _display: inline;">Abstract</label></p>
												<input type="checkbox" id="label_1_5"/>
												<div class="hidden_show">
													<h5>We develop Sequential Monte Carlo (SMC) algorithms exploiting numerical integrators which we show can lead to a new class of robust and efficient sampling algorithms.</h5>
												</div>
											</div>
											</td>
											</tr>											<tr>
												<td>17:00-19:00</td>
												<td>2-Hour Poster Session</td>
											</tr>

											<tr>
												<td colspan="2"><h2><b>26th March.</b></h2></td>
											</tr>
											<tr>
												<td>09:30-10:20</td>
												<td>
											<div class="hidden_box">
												<p>
												<a href="http://ibis.t.u-tokyo.ac.jp/suzuki/" target="_blank"><b>Taiji Suzuki</b></a> (The University of Tokyo / RIKEN AIP)
												</br>
												Title: TBA</br>
												<label for="label_2_1" style="display: inline-block; _display: inline;">Abstract</label></p>
												<input type="checkbox" id="label_2_1"/>
												<div class="hidden_show">
													<h5>
														TBA
													</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>10:30-11:20</td>
												<td>
											<div class="hidden_box">
												<p>
												<a href="http://www.gatsby.ucl.ac.uk/~laurence/" target="_blank"><b>Laurence Aitchison</b></a> (University of Bristol)
												</br>
												Title: Deep kernel machines and processes</br>
												<label for="label_2_2" style="display: inline-block; _display: inline;">Abstract</label>
												</p>
												<input type="checkbox" id="label_2_2"/>
												<div class="hidden_show">
													<h5>
														The successes of modern deep neural networks (DNNs) are founded on their ability to transform inputs across multiple layers to build good high-level representations. It is therefore critical to understand this process of representation learning. However, standard theoretical approaches involving infinite width limits give very limited insights into representation learning. For instance, the NNGP infinite-width limit entirely eliminates representation learning. Alternatively, mu-P just tells us whether or not representation learning is possible, without telling us anything about the representations that are actually learned. We therefore develop a new infinite width limit, the Bayesian representation learning limit, that exhibits representation learning mirroring that in finite-width networks, yet at the same time, remains extremely tractable. This limit gives rise to an elegant objective that describes how learning shapes representations at every layer. Using this objective, we develop a new, scalable family of "deep kernel methods", which are based on an infinite-width limit of deep Gaussian processes.  In practice, deep kernel methods just use kernels without ever using any features or weights. We develop a convolutional variant, known as Convolutional Deep Kernel Machines, and push their performance to 94.1% on CIFAR-10 (the previous SOTA for kernel methods was 91.2%, from Adlam et al. 2023)</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>11:30-12:20</td>
												<td>
											<div class="hidden_box">
												<p>
												<a href="https://scholar.google.com/citations?user=SDhzsXUAAAAJ&hl=en" target="_blank"><b>Juliette Unwin</b></a> (University of Bristol) 
												</br>
												Title: Using Hawkes Processes to model malaria in near elimination settings</br>
												<label for="label_2_2" style="display: inline-block; _display: inline;">Abstract</label>
												</p>
												<input type="checkbox" id="label_2_2"/>
												<div class="hidden_show">
													<h5>
														Globally there were an estimated 249 million malaria cases and 608,000 malaria deaths in 85 countries during 2022, predominantly in Africa, with 34 countries reporting fewer than 1000 indigenous cases of the disease. Modelling malaria in low transmission settings is challenging because prohibitively large sample sizes are needed to use traditional gold standard measures such as parasite prevalence.  Instead, we propose using Hawkes Processes to capture malaria disease dynamics in countries that are close to eliminating malaria. Our model combines malaria specific information, such as the shape of the infectious profile, within a rigorous statistical framework to fit incidence data. We show that it is possible to accurately recreate the case counts over time with our Hawkes Process method.  We also show that we can estimate the proportion of cases that are imported without using this information in our fitting process. </h5>
												</div>
											</div>
											</td>
											</tr>											<tr>
												<td>12:20-14:00</td>
												<td>Lunch break</td>
											</tr>
											<tr>
												<td>14:00-14:50</td>
												<td>
											<div class="hidden_box">
												<p>
												<a href="https://sethrf.com" target="_blank"><b>Seth Flaxman</b></a> (University of Oxford)
												</br>
												Title: Distribution regression, ecological inference, encoding GP aggregates and the change-of-support problem</br>
												<label for="label_2_3" style="display: inline-block; _display: inline;">Abstract</label>
                                                </p>
												<input type="checkbox" id="label_2_3"/>
												<div class="hidden_show">
													<h5>
														I will revisit work connecting distribution regression with kernel mean embeddings and ecological inference [Flaxman et al, KDD 2015; Law et al, ICML 2018] and discuss new work on efficient inference for aggregated GPs: "Deep learning and MCMC with aggVAE for shifting administrative boundaries: mapping malaria prevalence in Kenya" https://arxiv.org/abs/2305.19779 using deep generative modelling. I will conclude by discussing the connections between distribution regression and aggregated GPs ("Aggregated Gaussian Processes with Multiresolution Earth Observation Covariates,” https://arxiv.org/abs/2105.01460). </h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>15:00-15:50</td>
												<td>
											<div class="hidden_box">
												<p>
												<a href="https://mkolar.coffeejunkies.org" target="_blank"><b>Mladen Kolar</b></a> (University of Chicago)
												</br>
												Title: Adaptive Stochastic Optimization with Constraints </br>
												<label for="label_2_4" style="display: inline-block; _display: inline;">Abstract</label></p>
												<input type="checkbox" id="label_2_4"/>
												<div class="hidden_show">
													<h5>
														Constrained stochastic optimization problems appear widely in numerous applications in statistics, machine learning, and engineering, including constrained maximum likelihood estimation, constrained deep neural networks, physical-informed machine learning, and optimal control. I will discuss our recent work on solving nonlinear optimization problems with stochastic objective and deterministic constraints. I will describe development of adaptive algorithms based on sequential quadratic programming and their properties.  The talk is based on the joint work with Yuchen Fang, Ilgee Hong, SenNa, Michael Mahoney, and Mihai Anitescu.
													</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>16:00-18:00</td>
												<td>2-Hour Poster Session</td>
											</tr>
											<tr>
												<td colspan="2"><h2><b>27th March.</b></h2></td>
											</tr>
											<tr>
												<td>09:30-10:20</td>
												<td>
											<div class="hidden_box">
												<p>
												<a href="https://fxbriol.github.io" target="_blank"><b>François-Xavier Briol</b></a> (University College London)
												</br>
												Title: Robust and conjugate Gaussian Process regression
											    </br>
												<label for="label_3_1" style="display: inline-block; _display: inline;">Abstract</label></p>
												<input type="checkbox" id="label_3_1"/>
												<div class="hidden_show">
													<h5>To enable closed form conditioning, a common assumption in Gaussian process (GP) regression is independent and identically distributed Gaussian observation noise. This strong and simplistic assumption is often violated in practice, which leads to unreliable inferences and uncertainty quantification. Unfortunately, existing methods for robustifying GPs break closed-form conditioning, which makes them less attractive to practitioners and significantly more computationally expensive. In this talk, we demonstrate how to perform provably robust and conjugate Gaussian process (RCGP) regression at virtually no additional cost using generalised Bayesian inference. RCGP is particularly versatile as it enables exact conjugate closed form updates in all settings where standard GPs admit them. To demonstrate its strong empirical performance, we deploy RCGP for problems ranging from Bayesian optimisation to sparse variational Gaussian processes.</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>10:30-11:20</td>
												<td>
											<div class="hidden_box">
												<p>
												<a href="https://www.imperial.ac.uk/people/s.filippi" target="_blank"><b>Sarah Filipi</b></a>  (Imperial College London)
												</br>
												Title: Mixed-Output Gaussian Process Latent Variable Models: an application to pharmaceutical manufacturing.</br>
												<label for="label_3_5" style="display: inline-block; _display: inline;">Abstract</label>
												</p>
												<input type="checkbox" id="label_3_5"/>
												<div class="hidden_show">
													<h5>
														In this talk, I will present a  Bayesian non-parametric approach  for signal separation where the signals may vary according to latent variables. This method was motivated by applications in pharmaceutical manufacturing and is particularly relevant in spectroscopy, where changing conditions may cause the underlying pure component signals to vary from sample to sample. The key contribution is to augment Gaussian Process Latent Variable Models (GPLVMs) to incorporate the case where each data point comprises the weighted sum of a known number of pure component signals, observed across several input locations.To demonstrate the applicability to both spectroscopy and other domains, we consider several applications: a near-infrared spectroscopy data set with varying temperatures, a simulated data set for identifying flow configuration through a pipe, and a data set for determining the type of rock from its reflectance.</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>11:30-12:20</td>
												<td>
											<div class="hidden_box">
												<p>
												<a href="https://sites.google.com/view/mimaizumi/home" target="_blank"><b>Masaaki Imaizumi</b></a> (The University of Tokyo / RIKEN AIP)
												</br>
												Title: Statistical Analysis on Overparameterized Models and In-Context Learning</br>
												<label for="label_3_3" style="display: inline-block; _display: inline;">Abstract</label>
                                                </p>
												<input type="checkbox" id="label_3_3"/>
												<div class="hidden_show">
													<h5>
														Deep learning and artificial intelligence technologies, one of the modern data science technologies, have made great progress, and their mathematical understanding is required to efficiently control and develop these technologies. In this talk, we present two types of research related to this topic. (I) The first is high-dimensional statistics for excess parameter models as typified by large-scale neural networks. Traditional high-dimensional statistics has developed a methodology to reduce excess dimension. However, since recent large-degree-of-freedom models do not have explicit excess dimension, another theoretical approach has been developed in recent years. We present several results on the application of this approach to more practical statistical models. (II) The second is a statistical analysis of a scheme called in-context learning, which explains foundation models for artificial intelligence such as ChatGPT. We argue that in-context learning can achieve efficient learning under certain conditions, owing to the property of the transformer, which can handle the entire property of empirical distributions.</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>12:20-14:00</td>
												<td>Lunch break</td>
											</tr>
											<tr>
												<td>14:00-14:50</td>
												<td>
											<div class="hidden_box">
												<p>
												<a href="https://0-scholar-google-com.brum.beds.ac.uk/citations?user=juZXbFoAAAAJ&hl=no" target="_blank"><b>Jiaxin Shi</b></a>  (Deepmind)
												</br>
												Title: TBA</br>
												<label for="label_3_4" style="display: inline-block; _display: inline;">Abstract</label>
												</p>
												<input type="checkbox" id="label_3_4"/>
												<div class="hidden_show">
													<h5>TBA</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>15:00-15:50</td>
												<td>
											<div class="hidden_box">
												<p>
												<a href="https://www.gatsby.ucl.ac.uk/~gretton/" target="_blank"><b>Atrhur Gretton</b></a>  (University College London)
												</br>
												Title: Learning to act in noisy contexts using deep proxy learning 	</br>
												<label for="label_3_6" style="display: inline-block; _display: inline;">Abstract</label>
												</p>
												<input type="checkbox" id="label_3_6"/>
												<div class="hidden_show">
													<h5>
														We consider problem of evaluating the expected outcome of an action or policy, using off-policy observations of user actions, where the relevant context is noisy/anonymized. This scenario might arise due to privacy constraints, data bandwidth restrictions, or both.  We will employ the recently developed tool of proxy causal learning to address this problem. In brief, two noisy views of the context are used: one prior to the user action, and one subsequent to it, and influenced by the action. This pair of views will allow us to provably recover the average causal effect of an action under reasonable assumptions. As a key benefit of the proxy approach, we need never explicitly model or recover the hidden context. Our implementation employs learned neural net representations for both the action and context, allowing each to be complex and high dimensional (images, text). We demonstrate the deep proxy learning method in a setting where the action is an image, and show that we outperform an autoencoder-based alternative. </h5>
												</div>
											</div>
											</td>
											</tr>

											<tr>
												<td>18:00-20:00</td>
												<td>
											<div class="hidden_box">
												<p>
												<a href="https://www.lidobristol.com/" target="_blank"><b>Workshop Dinner</b></a>  (for Speakers)
												</br>
												</br>
												<label for="label_3_6" style="display: inline-block; _display: inline;"></label>
												</p>
												<input type="checkbox" id="label_3_6"/>
												<div class="hidden_show">
													<h5>
														TBA</h5>
												</div>
											</div>
											</td>
											</tr>

										</tbody>
									</table>
								</div>
							
								</section>
							<section id="organizers" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Organizers</h2>
										</header>
											<ul class="alt2">
											<li>
												Song Liu, University of Bristol
											</li>
											<li>
												François-Xavier Briol, University College London
											</li>
											<li>
												Masaaki Imaizumi, The University of Tokyo
											</li>
											<li>
												Taiji Suzuki, The University of Tokyo
											</li>
											<li>
												Kenji Fukumizu, The Institute of Statistical Mathematics
											</li>
											</ul>
									</div>
								</div>	
									
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Sponsors</h2>
										</header>
											<p>This workshop is supported by the following institutions and grants:
											<li>
												<a href="https://heilbronn.ac.uk" target="_blank">Heilbronn Institute for Mathematical Research</a>
											</li>
											<li>
												<a href="https://www.bristol.ac.uk/cdt/compass/" target="_blank">EPSRC Centre for Doctoral Training in Computational Statistics and Data Science</a>
											</li>
											<li>
												<a href="https://www.lms.ac.uk/grants/conference-grants-scheme-1" target="_blank">London Mathematics Society, Conference Grant</a>
											</li>
											<li>
												<a href="https://www.bristol.ac.uk/maths/" target="_blank">School of Mathematics, University of Bristol</a>
											</li>
											<li>
												<a href="https://www.bristolmathsresearch.org/statistical-science/" target="_blank">Institute for Statistical Science</a>
											</li>
											<li>
												<a href="http://www.ism.ac.jp/noe/sml-center/en/index.html" target="_blank">Research Center for Statistical Machine Learning, The Institute of Statistical Mathematics</a>
											</li>
											<li>
												<a href="https://www.nedo.go.jp/english/index.html" target="_blank">Japan Science and Technology Agency, CREST</a>
											</li>
												<ul style="margin: 0 0 0 0;list-style-type: none;"><li style="line-height:20px;"><span style="font-size : smaller;">
												"Innovation of Deep Structured Models with Representation of Mathematical Intelligence" 
												in 
												"Creating information utilization platform by integrating mathematical and information sciences, and development to society"</span>
											</li></ul>
											</p>
									</div>
								</div>
							</section>
					

							<section id="location" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Location</h2>
										</header>		

										<!-- table>
										<tr>
										<td width="50%">

										<iframe src="https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d2485.9790590763732!2d-2.604750723613292!3d51.45854091427176!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x48718ddbdfd292fb%3A0x2f0b60f89b4b6d56!2sUniversity%20of%20Bristol!5e0!3m2!1sen!2sjp!4v1693923807280!5m2!1sen!2sjp" width="100%" height="300" style="border:0;" allowfullscreen="" loading="lazy" referrerpolicy="no-referrer-when-downgrade" align="middle"></iframe>
										</td>
										<td>
										<p>Address: University of Bristol</br>
										Beacon House, Queens Rd, Bristol BS8 1QU, United Kingdom.</br>
										For detail, see <a href="https://www.bristol.ac.uk/maps/google/?latlng=51.457389,-2.604043&t=0&h=1" target="_blank">official access information</a>.</p>	
										</td>
										</tr>
										</table -->
										<p>Address: LG.02, Fry Building</br>
										University of Bristol, Woodland Rd, Bristol BS8 1UG, United Kingdom.</br>
										</p>
										<iframe src="https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d2486.045607263715!2d-2.606831623064941!3d51.457319071802054!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x48718dd973006a7f%3A0x944e3484310bc33e!2sFry%20Building%20School%20of%20Mathematics!5e0!3m2!1sen!2suk!4v1703014767246!5m2!1sen!2suk" width="600" height="450" style="border:0;" allowfullscreen="" loading="lazy" referrerpolicy="no-referrer-when-downgrade"></iframe>										
									</div>
								</div>
							</section>


					</div>

				<!-- Footer
					<footer id="footer">
						<p class="copyright">Copyright &copy; At?l?m Gune? Baydin. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
					</footer>  -->
			</div>

		<!-- Scripts  -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="assets/js/main.js"></script>
	</body>
</html>

