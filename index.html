<!DOCTYPE HTML>
<!--
	Stellar by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
		<!-- Global Site Tag (gtag.js) - Google Analytics -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=UA-79453179-2"></script>
		<script>
		  window.dataLayer = window.dataLayer || [];
		  function gtag(){dataLayer.push(arguments)};
		  gtag('js', new Date());

		  gtag('config', 'UA-79453179-2');
		</script>

		<title>Workshop on Functional Inference and Machine Intelligence 2021</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="assets/css/main.css" />
		<!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
	</head>
	<body>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header" class="alt">
						<!-- <span class="logo"><img src="images/logo.svg" alt="" /></span> -->
						<h1 style="color:white">Workshop on Functional Inference and Machine Intelligence</h1>
						<h3 style="color:white">Virtual, Anywhere on the Earth.</br>March 29-31, 2022.</h3>

					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul>
							<li><a href="#home" class="active">Home</a></li>
							<li><a href="#program">Program</a></li>
<!--							<li><a href="#schedule">Schedule</a></li>-->
							<li><a href="#organizers">Organizers</a></li>
							<li><a href="#location">Access</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Introduction -->
							<section id="home" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Home</h2>
										</header>
										<p>The <strong>Workshop on Functional Inference and Machine Intelligence (FIMI)</strong> is an international workshop on machine learning and statistics, with a particular focus on theory, methods, and practice. It consists of invited talks with topics including (but not limited):</p>
										<ul>
											<li>Machine Learning Methods</li>
											<li>Deep Learning</li>
											<li>Kernel Methods</li>
											<li>Probabilistic Methods</li>
										</ul>
										<p>The workshop will be a virtual (via Zoom), March 2021. All schedules are in Japan Standard Time (GMT+9).</p>
										<div Align="center">
										
										<!--<a href="./program.pdf" target="_blank" class="square_btn">
											<span>Booklet</br>(pdf)</span>
										</a>-->
										
										<!--
										<p><b>Please register! Links for the talks and the event will be emailed to registered participants.</b></p>
										<a href="https://docs.google.com/forms/d/e/1FAIpQLSfnqeyEyBH8jXDCQ5WPNxvvd1YmXhfKefnlwj__6rfr0RHOSQ/viewform" target="_blank" class="square_btn">
											<span>Registration</span>
										</a>
										&nbsp;&nbsp;
										<a href="https://docs.google.com/forms/d/e/1FAIpQLSdre-3VL0fhkRTbbi1o-1zGaO4K6mPAyD7f3KwN9msA9EFk9g/viewform" target="_blank" class="square_btn2">
											<span>Banquet</br>Application</span>
										</a>
										<p>To attend our <b>workshop</b>, please register your information. To attend the <b>banquet</b>, please apply <u>by 9th Feb.</u></p>
										<p>Thank you for a lot of registration.</p>
										-->
										<p>Previous Workshop: 
											<a href="https://sites.google.com/site/2016pgm/" target="_blank">2016</a>, 
											<a href="https://sites.google.com/site/2017pgm/home" target="_blank">2017</a>, 
											<a href="https://ismseminar.github.io/fimi2018/" target="_blank">2018</a>, 
											<a href="https://ismseminar.github.io/fimi2019/" target="_blank">2019</a>,
											<a href="https://ismseminar.github.io/fimi2020/" target="_blank">2020</a>
											<a href="https://ismseminar.github.io/fimi2021/" target="_blank">2021</a>
											<img src='images/IMG_1213.JPG' alt='' width="40%" align="middle" hspace="5%"></p>
										</div>
									</div>
								</div>
							</section>

						
							<section id="program" class="main">
							<div class="content">
								<header class="major">
									<h2>Confirmed Invited Speakers</h2>
								</header>
								<p>We will invite more speakers.</p>
						<li>
						<p><a href="http://www.gatsby.ucl.ac.uk/~gretton/" target="_blank"><b>Arthur Gretton</b></a>  (University College London)</p>
						</li>
						<li>
						<p><a href="https://tyliang.github.io/Tengyuan.Liang/" target="_blank"><b>Tengyuan Liang</b></a>  (The University of Chicago)</p>
						</li>
						<li>
						<p><a href="http://ibis.t.u-tokyo.ac.jp/suzuki/" target="_blank"><b>Taiji Suzuki</b></a>  (University of Tokyo)</p>
						</li>
						<li>
						<p><a href="https://allmodelsarewrong.net" target="_blank"><b>Song Liu</b></a>  (University of Bristol)</p>
						</li>
						<li>
						<p><a href="http://www.ism.ac.jp/~fukumizu/" target="_blank"><b>Kenji Fukumizu</b></a>  (The Institute of Statistical Mathematics)</p>
						</li>
						<li>
						<p><a href="https://sites.google.com/view/mimaizumi/" target="_blank"><b>Masaaki Imaizumi</b></a>  (The University of Tokyo)</p>
						</li>
							</div>
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Program</h2>
										</header>
										<p>To be announced.</p>
									</div>
								</div>
						<!--
								<div class="table-wrapper">
									<table>
										<tbody>
											<tr>
												<td colspan="2"><h2><b>Tuesday 2nd March.</b></h2></td>
											</tr>
											<tr>
												<td>09:55–10:00</td>
												<td>Opening</td>
											</tr>
											<tr>
												<td>10:00-11:00</td>
												<td>
											<div class="hidden_box">
												<p><a href="https://www.cs.toronto.edu/~dennywu/" target="_blank"><b>Denny Wu</b></a> (University of Toronto)</br>
												Title: Explicit and Implicit Regularization in Overparameterized Least Squares Regression </br><label for="label_1_1" style="display: inline-block; _display: inline;">Abstract</label>&emsp;<a href="./slides/wu.pdf" target="_blank"><label style="display: inline-block; _display: inline;">Slide</label></a></p>
												<input type="checkbox" id="label_1_1"/>
												<div class="hidden_show">
													<h5>We study the generalization properties of the generalized ridge regression estimator in the overparameterized regime. We derive the exact prediction risk (generalization error) in the proportional asymptotic limit, and decide the optimal weighted L2 penalty. Our result provides a rigorous characterization of the surprising phenomenon that the optimal ridge regularization strength can be *negative*. We then connect the ridgeless limit of this estimator to the implicit bias of preconditioned gradient descent (e.g., natural gradient descent); this allows us to compare the generalization performance of first- and second-order optimizers, and identify different factors that affect this comparison. Our theoretical finding also aligns with empirical observation in various neural network experiments.</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>11:20-12:20</td>
												<td>
											<div class="hidden_box">
												<p><a href="https://sites.google.com/view/mimaizumi/" target="_blank"><b>Masaaki Imaizumi</b></a> (The University of Tokyo)</br>
												Title: Generalization Analysis of Deep Models with Loss Surface and Likelihood Models </br><label for="label_1_2" style="display: inline-block; _display: inline;">Abstract</label></p>
												<input type="checkbox" id="label_1_2"/>
												<div class="hidden_show">
													<h5>Data analysis using large models such as deep learning has high generalization performance, however its principles are still unclear. In this talk, we will propose two theoretical frameworks to explain the principle. First, we develop a regularization theory using a shape of loss surfaces. The generalization error evaluation using uniform convergence has been questioned for its validity of assumptions. To resolve the question, we show that a minimum of loss surfaces in the population sence realizes regularization. Second, we develop theory of the double descent phenomenon, where a generalization error decreases in the limit of large parameters. Despite its generality, applicability of it are limited to shallow neural networks. In this study, we show that it be applied to a wide range of maximum likelihood models, including deep models as well.</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>12:20–14:00</td>
												<td>Lunch break</td>
											</tr>
											<tr>
												<td>14:00-15:00</td>
												<td>
											<div class="hidden_box">
												<p><a href="http://ibis.t.u-tokyo.ac.jp/suzuki/" target="_blank"><b>Taiji Suzuki</b></a> (The University of Tokyo)</br>
												Title: Optimization and statistical efficiency of neural network in mean field regimes</br><label for="label_1_3" style="display: inline-block; _display: inline;">Abstract</label></p>
												<input type="checkbox" id="label_1_3"/>
												<div class="hidden_show">
													<h5>In this talk, I discuss optimization of neural network in mean field regimes and show its statistical efficiency with optimization guarantees. First, I present a deep learning optimization framework based on a noisy gradient descent in an infinite dimensional Hilbert space (gradient Langevin dynamics), and show generalization error and excess risk bounds for the solution obtained by the optimization procedure. I will show that deep learning can avoid the curse of dimensionality in a teacher-student setting, and eventually achieve better excess risk than kernel methods. Next, I discuss identifiability of neural network with a gradient descent method under a teacher-student setting. It will be shown that with a sparse regularization, we can show that the measure representation of the student network converges to that of the teacher network, while convergence in terms of parameters will not be guaranteed. Finally, I will briefly introduce a dual averaging method to optimize the neural network with a mean field representation and discuss its convergence. The proposed method utilizes a gradient Langevin dynamics and is guaranteed to converge the global optimal solution.</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>15:20-16:20</td>
												<td>
											<div class="hidden_box">
												<p><a href="https://sites.google.com/view/ryokarakida/" target="_blank"><b>Ryo Karakida</b></a> (National Institute of Advanced Industrial Science and Technology)</br>
												Title: Analysis of Fisher information and natural gradient descent in infinitely wide neural networks</br><label for="label_1_4" style="display: inline-block; _display: inline;">Abstract</label></p>
												<input type="checkbox" id="label_1_4"/>
												<div class="hidden_show">
													<h5>Deep neural networks with random weights give us some insight into the typical behavior of networks or learning dynamics around random initialization. In particular, the neural tangent kernel (NTK) regime can hold for sufficiently wide networks and enables us to solve the learning dynamics explicitly. Following this line of analyses, we first investigate the local geometric structure of the loss surface characterized by Fisher information. Fisher information and NTK share the same eigenvalues which determine some convergence properties of training. Second, we analyze the dynamics of natural gradient descent (NGD) in the NTK regime. While NGD is known to accelerate the convergence of training, it requires high computational cost, and thus we usually use its approximation in practice.  We prove that typical approximation methods such as K-FAC and unit-wise Fisher can achieve the same convergence rate as exact NGD.</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>16:40-17:40</td>
												<td>
											<div class="hidden_box">
												<p><a href="https://lchizat.github.io" target="_blank"><b>Lénaïc Chizat</b></a> (Université Paris-Saclay)</br>
												Title: Analysis of Gradient Descent on Wide Two-Layer ReLU Neural Networks</br><label for="label_1_5" style="display: inline-block; _display: inline;">Abstract</label>&emsp;<a href="./slides/chizat.pdf" target="_blank"><label style="display: inline-block; _display: inline;">Slide</label></a></p>
												<input type="checkbox" id="label_1_5"/>
												<div class="hidden_show">
													<h5>Artificial neural networks are a family of parametric models which, given a training set of sample/label pairs, can be trained to predict the labels of new samples. To do so, the training algorithm updates the parameters using variants of the gradient descent method on a well-chosen objective function (the empirical risk, with potentially a regularization term). In this talk, we propose an analysis of gradient descent on wide two-layer ReLU neural networks (networks with many parameters but only one simple "positive part" non-linearity) that leads to sharp characterizations of the learned predictor. The main idea is to study the dynamics when the width of the neural network goes to infinity, which can be written as a Wasserstein gradient flow. While this dynamics evolves on a non-convex landscape, we show that when the parameters are initialized properly, its limit is a global minimizer. We also study the "implicit bias" of this algorithm in various situations: among all the minimizers, we show that it selects a specific one which depends, among other things, on the initialization and the choice of objective function. Along the way, we discuss what these results tell us about the statistical performance of these models. This is based on joint work with Francis Bach.</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>19:00–21:00</td>
												<td>Online Networking Event</td>
											</tr>
											<tr>
												<td colspan="2"><h2><b>Wednesday 3rd March.</b></h2></td>
											</tr>
											<tr>
												<td>10:00-11:00</td>
												<td>
											<div class="hidden_box">
												<p><a href="https://www.mi.t.u-tokyo.ac.jp/mukuta/" target="_blank"><b>Yusuke Mukuta</b></a>  (The University of Tokyo)</br>
												Title: Feature coding using invariance and kernel approximation</br><label for="label_2_1" style="display: inline-block; _display: inline;">Abstract</label></p>
												<input type="checkbox" id="label_2_1"/>
												<div class="hidden_show">
													<h5>Feature coding is a method to use the statistics of the local features as a global feature, which can be used to enhance the performance of Convolutional Neural Networks for the image recognition task. We introduce two novel feature coding methods. First is a method to exploit the invariance of the input image such as rotation invariance and flip invariance for the feature coding function using the idea of tensor product representation. Second is a method to construct a compact approximation for the existing coding function using the technique of kernel approximation. These methods contribute to the high recognition accuracy with small feature dimension.</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>11:20-12:20</td>
												<td>
											<div class="hidden_box">
												<p><a href="https://takeshi-teshima.github.io" target="_blank"><b>Takeshi Teshima</b></a>  (The University of Tokyo) / <a href="https://researchmap.jp/1sa014kawa?lang=en" target="_blank"><b>Isao Ishikawa</b></a>  (Ehime University)</br>
												Title: On the Universality of Invertible Neural Networks</br><label for="label_2_2" style="display: inline-block; _display: inline;">Abstract</label>&emsp;<a href="./slides/teshima.pdf" target="_blank"><label style="display: inline-block; _display: inline;">Slide</label></a></p>
												<input type="checkbox" id="label_2_2"/>
												<div class="hidden_show">
													<h5>Invertible neural networks (INNs) are neural network architectures with invertibility by design. Thanks to their invertibility and the tractability of Jacobian, INNs have found various machine learning applications such as probabilistic modeling and feature extraction. However, their attractive properties come at the cost of restricting the layer designs, which poses a question on their representation power: can we use these models to approximate sufficiently diverse functions? In this research, we developed a general theoretical framework to investigate the representation power of INNs, building on a structure theorem of differential geometry. More specifically, the framework allows us to show the universal approximation properties of INNs for approximating a large class of diffeomorphisms. We applied the framework to two representative examples of INNs, namely Coupling-Flow-based INNs (CF-INNs) and Neural Ordinary Differential Equations (NODEs), and elucidated their high representation power despite their restricted architectures.</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>12:20–14:00</td>
												<td>Lunch break</td>
											</tr>
											<tr>
												<td>14:00-15:00</td>
												<td>
											<div class="hidden_box">
												<p><a href="http://www.ism.ac.jp/~fukumizu/" target="_blank"><b>Kenji Fukumizu</b></a>  (The Institute of Statistical Mathematics)</br>
												Title: Robust Topological Data Analysis using Reproducing Kernels</br><label for="label_2_3" style="display: inline-block; _display: inline;">Abstract</label>&emsp;<a href="https://www.dropbox.com/s/udutiup95jkmn88/FIMI2021_fukumizu.pdf?dl=0" target="_blank"><label style="display: inline-block; _display: inline;">Slide</label></a></p>
												<input type="checkbox" id="label_2_3"/>
												<div class="hidden_show">
													<h5>Persistent homology has become an important tool for extracting geometric and topological features from data, whose multi-scale features are summarized in a persistence diagram. From a statistical perspective, however, persistence diagrams are very sensitive to perturbations in the input space. In this work, we develop a framework for constructing robust persistence diagrams from superlevel filtrations of robust density estimators constructed using reproducing kernels. Using an analogue of the influence function on the space of persistence diagrams, we establish the proposed framework to be less sensitive to outliers. The robust persistence diagrams are shown to be consistent estimators in bottleneck distance, with the convergence rate controlled by the smoothness of the kernel---this in turn allows us to construct uniform confidence bands in the space of persistence diagrams. Finally, we demonstrate the superiority of the proposed approach on benchmark datasets.</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>15:20-16:20</td>
												<td>
											<div class="hidden_box">
												<p><a href="https://pierrealquier.github.io" target="_blank"><b>Pierre Alquier</b></a>  (RIKEN)</br>
												Title: Parametric estimation via MMD optimization: robustness to outliers and dependence</br><label for="label_2_4" style="display: inline-block; _display: inline;">Abstract</label></p>
												<input type="checkbox" id="label_2_4"/>
												<div class="hidden_show">
													<h5>In this talk,  I  will  study  the  properties  of  parametric  estimators  based  on  the  Maximum Mean Discrepancy (MMD) defined by Briol et al.  (2019).  In a first time,  I will show that these estimators are universal in the i.i.d setting: even in case of misspecification, they converge to the best approximation of the distribution of the data in the model, without ANY assumption on this model. This leads to very strong robustness properties. In a second time,I will show that these results remain valid when the data is not independent,  but satisfy instead a weak-dependence condition. This condition is based on a new dependence coefficient, which is itself defined thanks to the MMD. I will show through examples that this new notion of dependence is actually quite general.  This talk is based on the following papers and softwares, with Badr-Eddine Chérief Abdellatif (Oxford University), Mathieu Gerber(University of Bristol), Jean-David Fermanian (ENSAE Paris) and Alexis Derumigny (University of Twente):
													<br><a href="http://arxiv.org/abs/1912.05737" target="_blank">http://arxiv.org/abs/1912.05737</a>
													<br><a href="http://proceedings.mlr.press/v118/cherief-abdellatif20a.html" target="_blank">http://proceedings.mlr.press/v118/cherief-abdellatif20a.html</a>
													<br><a href="http://arxiv.org/abs/2006.00840" target="_blank">http://arxiv.org/abs/2006.00840</a>
													<br><a href="https://arxiv.org/abs/2010.00408" target="_blank">https://arxiv.org/abs/2010.00408</a>
													<br><a href="https://cran.r-project.org/web/packages/MMDCopula/" target="_blank">https://cran.r-project.org/web/packages/MMDCopula/</a></h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>16:40-17:40</td>
												<td>
											<div class="hidden_box">
												<p><a href="http://www.krikamol.org" target="_blank"><b>Krikamol Muandet</b></a>  (Max Planck Institute for Intelligent Systems)</br>
												Title: Maximum Moment Restriction and Its Applications</br><label for="label_2_5" style="display: inline-block; _display: inline;">Abstract</label></p>
												<input type="checkbox" id="label_2_5"/>
												<div class="hidden_show">
													<h5>In this talk, I will discuss statistical inference and hypothesis testing on models that are specified by conditional moment restrictions (CMR). Our approach employs a maximum moment restriction (MMR) which is constructed by maximising the interaction between the generalised residual function and functions of the conditioning variables that belong to a unit ball of a vector-valued reproducing kernel Hilbert space (vv-RKHS). The reproducing kernel induces the information geometry on the parameter space from which we can choose the parameter estimates so that the sample-based MMR is zero. The MMR allows for an infinite continuum of moment restrictions to be used, while permitting a tractable objective function for both parameter estimation and hypothesis testing.</h5>
												</div>
											</div>
											</td>
											</tr>

										</tbody>
									</table>
								</div>
							-->
								</section>


							<section id="organizers" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Organizers</h2>
										</header>
											<li>
												<p>Masaaki Imaizumi</br>
												Associate Professor, The University of Tokyo</p>
											</li>
											<li>
												<p>Taiji Suzuki</br>
												Associate Professor, The University of Tokyo</p>
											</li>
											<li>
												<p>Kenji Fukumizu</br>
												Professor, The Institute of Statistical Mathematics</p>
											</li>
											<p>Contact: please email to Masaaki Imaizumi (<a href="mailto:imaizumi@g.ecc.u-tokyo.ac.jp">imaizumi@g.ecc.u-tokyo.ac.jp</a>)</p>

											<p>This workshop is supported by the following institution and grant:
											<li>
												<a href="http://www.ism.ac.jp/noe/sml-center/en/index.html" target="_blank">Research Center for Statistical Machine Learning, the Institute of Statistical Mathematics</a>
											</li>
											<li>
												<a href="https://www.nedo.go.jp/english/index.html" target="_blank">Japan Science and Technology Agency, CREST</a>
											</li>
											<li>
												<a href="https://www.u-tokyo.ac.jp/en/" target="_blank">The University of Tokyo</a>
											</li>
											</p>
									</div>
								</div>
							</section>
					
							<section id="location" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Access Information</h2>
										</header>		

										<table>
										<tr>
										<p>The zoom link will be sent to all registered attendees via mail.</p>
										<!--<td width="50%">
										
										<img src="images/eurecom.jpg" width="100%" align="middle">
										</td>
										<td>
										<p>Address: EURECOM</br>
										Campus SophiaTech, 450 Route des Chappes, 06410 Biot, France.</br>
										<a href="https://sites.google.com/site/motonobukanagawa/eurecom" target="_blank">How to access</a>
										</p>	
											
										</td>-->
										</tr>
										</table>
										
							<!--<iframe src="https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d2888.623225593083!2d7.068936315496511!3d43.614385979122524!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x12cc2bbceb8ef3b9%3A0x22dae297f1be6add!2sEURECOM!5e0!3m2!1sen!2sjp!4v1574435390590!5m2!1sen!2sjp" width="100%" height="300em" frameborder="0" style="border:0" allowfullscreen></iframe>-->
									</div>
								</div>
							</section>

					</div>

				<!-- Footer
					<footer id="footer">
						<p class="copyright">Copyright &copy; Atılım Güneş Baydin. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
					</footer>  -->
			</div>

		<!-- Scripts  -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="assets/js/main.js"></script>
	</body>
</html>

