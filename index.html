<!DOCTYPE HTML>
<!--
	Stellar by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
		<!-- Global Site Tag (gtag.js) - Google Analytics -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=UA-79453179-2"></script>
		<script>
		  window.dataLayer = window.dataLayer || [];
		  function gtag(){dataLayer.push(arguments)};
		  gtag('js', new Date());

		  gtag('config', 'UA-79453179-2');
		</script>

		<title>Workshop on Functional Inference and Machine Intelligence 2021</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="assets/css/main.css" />
		<!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
	</head>
	<body>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header" class="alt">
						<!-- <span class="logo"><img src="images/logo.svg" alt="" /></span> -->
						<h1 style="color:white">Workshop on Functional Inference and Machine Intelligence</h1>
						<h3 style="color:white">Virtual, Anywhere on the Earth.</br>March 2-3, 2021.</h3>

					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul>
							<li><a href="#home" class="active">Home</a></li>
							<li><a href="#program">Program</a></li>
<!--							<li><a href="#schedule">Schedule</a></li>-->
							<li><a href="#organizers">Organizers</a></li>
							<li><a href="#location">Access</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Introduction -->
							<section id="home" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Home</h2>
										</header>
										<p>The <strong>Workshop on Functional Inference and Machine Intelligence (FIMI)</strong> is an international workshop on machine learning and statistics, with a particular focus on theoretical and algorithmic aspects. It consists of invited talks with topics including (but not limited):</p>
										<ul>
											<li>Mathematical Analysis of Deep Learning</li>
											<li>Kernel and Probabilistic Models in Machine Learning</li>
											<li>Statistical Learning Theory</li>
										</ul>
										<p>The workshop will be a virtual (via Zoom), 2-3, March 2021. Every schedule follows Japan Standard Time (JST).</p>
										<div Align="center">
										
										<!--<a href="./program.pdf" target="_blank" class="square_btn">
											<span>Booklet</br>(pdf)</span>
										</a>-->
										
								<!--<p><b>Registration is closed. If you have any questions, please contact the organizers.</b></p>-->
										<!--<a href="https://forms.gle/yntQARTLmkgR53TU8" target="_blank" class="square_btn">
											<span>Registration</span>
										</a>-->
										<!--
										&nbsp;&nbsp;
										<a href="https://docs.google.com/forms/d/e/1FAIpQLSdre-3VL0fhkRTbbi1o-1zGaO4K6mPAyD7f3KwN9msA9EFk9g/viewform" target="_blank" class="square_btn2">
											<span>Banquet</br>Application</span>
										</a>
										<p>To attend our <b>workshop</b>, please register your information. To attend the <b>banquet</b>, please apply <u>by 9th Feb.</u></p>
										<p>Thank you for a lot of registration.</p>
										-->
										<p>Previous Workshop: 
											<a href="https://sites.google.com/site/2016pgm/" target="_blank">2016</a>, 
											<a href="https://sites.google.com/site/2017pgm/home" target="_blank">2017</a>, 
											<a href="https://ismseminar.github.io/fimi2018/" target="_blank">2018</a>, 
											<a href="https://ismseminar.github.io/fimi2019/" target="_blank">2019</a>,
											<a href="https://ismseminar.github.io/fimi2020/" target="_blank">2020</a>
											<img src='images/IMG_1213.JPG' alt='' width="40%" align="middle" hspace="5%"></p>
										</div>
									</div>
								</div>
							</section>

							<section id="program" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Program</h2>
										</header>
									</div>
								</div>
								<div class="table-wrapper">
									<table>
										<tbody>
											<tr>
												<td colspan="2"><h2><b>Tuesday 2nd March.</b></h2></td>
											</tr>
											<tr>
												<td>09:55–10:00</td>
												<td>Opening</td>
											</tr>
											<tr>
												<td>10:00-11:00</td>
												<td>
											<div class="hidden_box">
												<p><a href="https://www.cs.toronto.edu/~dennywu/" target="_blank"><b>Denny Wu</b></a> (University of Toronto)</br>
												Title: Explicit and Implicit Regularization in Overparameterized Least Squares Regression </br><label for="label_1_1" style="display: inline-block; _display: inline;">Abstract</label></p>
												<input type="checkbox" id="label_1_1"/>
												<div class="hidden_show">
													<!--hidden-->     
													<h5>We study the generalization properties of the generalized ridge regression estimator in the overparameterized regime. We derive the exact prediction risk (generalization error) in the proportional asymptotic limit, and decide the optimal weighted L2 penalty. Our result provides a rigorous characterization of the surprising phenomenon that the optimal ridge regularization strength can be *negative*. We then connect the ridgeless limit of this estimator to the implicit bias of preconditioned gradient descent (e.g., natural gradient descent); this allows us to compare the generalization performance of first- and second-order optimizers, and identify different factors that affect this comparison. Our theoretical finding also aligns with empirical observation in various neural network experiments.</h5>
													<!--hidden-->
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>11:20-12:20</td>
												<td>
											<div class="hidden_box">
												<p><a href="https://sites.google.com/view/mimaizumi/" target="_blank"><b>Masaaki Imaizumi</b></a> (The University of Tokyo)</br>
												Title: Generalization Analysis of Deep Models with Loss Surface and Likelihood Models </br><label for="label_1_2" style="display: inline-block; _display: inline;">Abstract</label></p>
												<input type="checkbox" id="label_1_2"/>
												<div class="hidden_show">
													<!--hidden-->     
													<h5>Data analysis using large models such as deep learning has high generalization performance, however its principles are still unclear. In this talk, we will propose two theoretical frameworks to explain the principle. First, we develop a regularization theory using a shape of loss surfaces. The generalization error evaluation using uniform convergence has been questioned for its validity of assumptions. To resolve the question, we show that a minimum of loss surfaces in the population sence realizes regularization. Second, we develop theory of the double descent phenomenon, where a generalization error decreases in the limit of large parameters. Despite its generality, applicability of it are limited to shallow neural networks. In this study, we show that it be applied to a wide range of maximum likelihood models, including deep models as well.</h5>
													<!--hidden-->
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>12:20–14:00</td>
												<td>Lunch break</td>
											</tr>
											<tr>
												<td>14:00-15:00</td>
												<td>
											<div class="hidden_box">
												<p><a href="http://ibis.t.u-tokyo.ac.jp/suzuki/" target="_blank"><b>Taiji Suzuki</b></a> (The University of Tokyo)</br>
												Title: TBD</br><label for="label_1_3" style="display: inline-block; _display: inline;">Abstract</label></p>
												<input type="checkbox" id="label_1_3"/>
												<div class="hidden_show">
													<!--hidden-->     
													<h5>TBW</h5>
													<!--hidden-->
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>15:20-16:20</td>
												<td>
											<div class="hidden_box">
												<p><a href="https://sites.google.com/view/ryokarakida/" target="_blank"><b>Ryo Karakida</b></a> (National Institute of Advanced Industrial Science and Technology)</br>
												Title: TBD</br><label for="label_1_4" style="display: inline-block; _display: inline;">Abstract</label></p>
												<input type="checkbox" id="label_1_4"/>
												<div class="hidden_show">
													<!--hidden-->     
													<h5>TBW</h5>
													<!--hidden-->
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>16:40-17:40</td>
												<td>
											<div class="hidden_box">
												<p><a href="https://lchizat.github.io" target="_blank"><b>Lénaïc Chizat</b></a> (Université Paris-Saclay)</br>
												Title: Analysis of Gradient Descent on Wide Two-Layer ReLU Neural Networks</br><label for="label_1_5" style="display: inline-block; _display: inline;">Abstract</label></p>
												<input type="checkbox" id="label_1_5"/>
												<div class="hidden_show">
													<!--hidden-->     
													<h5>Artificial neural networks are a family of parametric models which, given a training set of sample/label pairs, can be trained to predict the labels of new samples. To do so, the training algorithm updates the parameters using variants of the gradient descent method on a well-chosen objective function (the empirical risk, with potentially a regularization term). In this talk, we propose an analysis of gradient descent on wide two-layer ReLU neural networks (networks with many parameters but only one simple "positive part" non-linearity) that leads to sharp characterizations of the learned predictor. The main idea is to study the dynamics when the width of the neural network goes to infinity, which can be written as a Wasserstein gradient flow. While this dynamics evolves on a non-convex landscape, we show that when the parameters are initialized properly, its limit is a global minimizer. We also study the "implicit bias" of this algorithm in various situations: among all the minimizers, we show that it selects a specific one which depends, among other things, on the initialization and the choice of objective function. Along the way, we discuss what these results tell us about the statistical performance of these models. This is based on joint work with Francis Bach.</h5>
													<!--hidden-->
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>19:00–21:00</td>
												<td>Online Networking Event</td>
											</tr>
											<tr>
												<td colspan="2"><h2><b>Wednesday 3rd March.</b></h2></td>
											</tr>
											<tr>
												<td>10:00-11:00</td>
												<td>
											<div class="hidden_box">
												<p><a href="https://www.mi.t.u-tokyo.ac.jp/mukuta/" target="_blank"><b>Yusuke Mukuta</b></a>  (The University of Tokyo)</br>
												Title: TBD</br><label for="label_2_1" style="display: inline-block; _display: inline;">Abstract</label></p>
												<input type="checkbox" id="label_2_1"/>
												<div class="hidden_show">
													<!--hidden-->     
													<h5>TBW</h5>
													<!--hidden-->
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>11:20-12:20</td>
												<td>
											<div class="hidden_box">
												<p><a href="https://takeshi-teshima.github.io" target="_blank"><b>Takeshi Teshima</b></a>  (The University of Tokyo) / <a href="https://researchmap.jp/1sa014kawa?lang=en" target="_blank"><b>Isao Ishikawa</b></a>  (Ehime University)</br>
												Title: On the Universality of Invertible Neural Networks</br><label for="label_2_2" style="display: inline-block; _display: inline;">Abstract</label></p>
												<input type="checkbox" id="label_2_2"/>
												<div class="hidden_show">
													<!--hidden-->     
													<h5>Invertible neural networks (INNs) are neural network architectures with invertibility by design. Thanks to their invertibility and the tractability of Jacobian, INNs have found various machine learning applications such as probabilistic modeling and feature extraction. However, their attractive properties come at the cost of restricting the layer designs, which poses a question on their representation power: can we use these models to approximate sufficiently diverse functions? In this research, we developed a general theoretical framework to investigate the representation power of INNs, building on a structure theorem of differential geometry. More specifically, the framework allows us to show the universal approximation properties of INNs for approximating a large class of diffeomorphisms. We applied the framework to two representative examples of INNs, namely Coupling-Flow-based INNs (CF-INNs) and Neural Ordinary Differential Equations (NODEs), and elucidated their high representation power despite their restricted architectures.</h5>
													<!--hidden-->
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>12:20–14:00</td>
												<td>Lunch break</td>
											</tr>
											<tr>
												<td>14:00-15:00</td>
												<td>
											<div class="hidden_box">
												<p><a href="http://www.ism.ac.jp/~fukumizu/" target="_blank"><b>Kenji Fukumizu</b></a>  (The Institute of Statistical Mathematics)</br>
												Title: TBD</br><label for="label_2_3" style="display: inline-block; _display: inline;">Abstract</label></p>
												<input type="checkbox" id="label_2_3"/>
												<div class="hidden_show">
													<!--hidden-->     
													<h5>TBW</h5>
													<!--hidden-->
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>15:20-16:20</td>
												<td>
											<div class="hidden_box">
												<p><a href="https://pierrealquier.github.io" target="_blank"><b>Pierre Alquier</b></a>  (RIKEN)</br>
												Title: TBD</br><label for="label_2_4" style="display: inline-block; _display: inline;">Abstract</label></p>
												<input type="checkbox" id="label_2_4"/>
												<div class="hidden_show">
													<!--hidden-->     
													<h5>TBW</h5>
													<!--hidden-->
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>16:20-17:20</td>
												<td>
											<div class="hidden_box">
												<p><a href="http://www.krikamol.org" target="_blank"><b>Krikamol Muandet</b></a>  (Max Planck Institute for Intelligent Systems)</br>
												Title: TBD</br><label for="label_2_5" style="display: inline-block; _display: inline;">Abstract</label></p>
												<input type="checkbox" id="label_2_5"/>
												<div class="hidden_show">
													<!--hidden-->     
													<h5>TBW</h5>
													<!--hidden-->
												</div>
											</div>
											</td>
											</tr>

										</tbody>
									</table>
								</div>
								</section>


							<section id="organizers" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Organizers</h2>
										</header>
											<li>
												<p>Masaaki Imaizumi</br>
												Associate Professor, The University of Tokyo</p>
											</li>
											<li>
												<p>Taiji Suzuki</br>
												Associate Professor, The University of Tokyo</p>
											</li>
											<li>
												<p>Kenji Fukumizu</br>
												Professor, The Institute of Statistical Mathematics</p>
											</li>
											<p>Contact: please email to Masaaki Imaizumi (<a href="mailto:imaizumi@g.ecc.u-tokyo.ac.jp">imaizumi@g.ecc.u-tokyo.ac.jp</a>)</p>

											<p>This workshop is supported by the following institution and grant:
											<li>
												<a href="http://www.ism.ac.jp/noe/sml-center/en/index.html" target="_blank">Research Center for Statistical Machine Learning, the Institute of Statistical Mathematics</a>
											</li>
											<li>
												<a href="https://www.nedo.go.jp/english/index.html" target="_blank">Japan Science and Technology Agency, CREST</a>
											</li>
											<li>
												<a href="https://www.u-tokyo.ac.jp/en/" target="_blank">The University of Tokyo</a>
											</li>
											</p>
									</div>
								</div>
							</section>
					
							<section id="location" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Access Information</h2>
										</header>		

										<table>
										<tr>
										<p>The zoom link will be sent to all registered attendees via mail.</p>
										<!--<td width="50%">
										
										<img src="images/eurecom.jpg" width="100%" align="middle">
										</td>
										<td>
										<p>Address: EURECOM</br>
										Campus SophiaTech, 450 Route des Chappes, 06410 Biot, France.</br>
										<a href="https://sites.google.com/site/motonobukanagawa/eurecom" target="_blank">How to access</a>
										</p>	
											
										</td>-->
										</tr>
										</table>
										
							<!--<iframe src="https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d2888.623225593083!2d7.068936315496511!3d43.614385979122524!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x12cc2bbceb8ef3b9%3A0x22dae297f1be6add!2sEURECOM!5e0!3m2!1sen!2sjp!4v1574435390590!5m2!1sen!2sjp" width="100%" height="300em" frameborder="0" style="border:0" allowfullscreen></iframe>-->
									</div>
								</div>
							</section>

					</div>

				<!-- Footer
					<footer id="footer">
						<p class="copyright">Copyright &copy; Atılım Güneş Baydin. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
					</footer>  -->
			</div>

		<!-- Scripts  -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="assets/js/main.js"></script>
	</body>
</html>

